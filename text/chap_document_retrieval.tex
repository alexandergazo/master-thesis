\chapter{Document Retrieval}
\label{chap:docret}

The term document retrieval refers to the task of finding relevant information to user queries in a large set of records (documents). 
% Therefore, the document retrieval system operates over a database of records, which for this thesis are described in \ref{chap:data}.
One can think of document retrieval as a search in a vast database of documents. 
In this view, web search services, such as Google, are a form of document retrieval. The database of documents are all accessible web pages on the internet, and the user query functions as the search query.

For our uses, the query is the claim to be fact-checked, and the document database is a collection of relevant documents, which we call knowledge scope. 

\section{Formal Description}
The task can be formally described \citep{two-tower} using a scoring function (sometimes referred to as ranking function) %article before ranking maybe
$$f\colon\mathcal{D}\times\mathcal{Q}\rightarrow\R$$
that maps a document-query pair $(d, q)$ to a score $f(d,q)$. 
Then, typically, the documents in pairs with the highest scores are considered to be the proposed solution to the task. 

This definition also fits a description of a range of other tasks such as open-domain question answering \citep{wiki-retrieval} or recommendation systems.

\section{Approaches to Document Retrieval}
% TODO two tower paper, 
In order to solve the document retrieval task, we will focus on finding an appropriate function $f$.
In this thesis, that means a function suitable for long documents.

This thesis explores both neural and non-neural (traditional) approaches to designing the scoring function, with emphasis on the neural approach while using the other as the baseline.

% This section introduces the most common methods of document retrieval, as well as new models with great potential.

\subsection{Traditional Approaches}

\subsubsection{TFIDF}

The traditional approaches are motivated by the intuition that relevant documents will contain the same words as those present in the query. 
Longer documents are at an advantage since there is a higher chance of the relevant words being present, so the term count is often normalized by the number of all terms in the document.
This simple metric is called term frequency (TF). 

TF can be ineffective if some of the terms in the query are very common in the document database.
This issue is resolved by introducing inverse document frequency, which informs how common the term is across all the documents.
The base version of IDF:
$$\text{idf}(t, D)=\log{\frac{\abs{D}}{\abs{\{d\in D: t\in d\}}}}\ .$$

% If we combine these two metrics
Multiplying these two metrics, we get the TFIDF weight of term $t$ in document $d$ in document database $D$:
%which is calculated simply as
$$\text{tfidf}(t,d,D)=\text{tf}(t,d)\cdot\text{idf}(t,D)\ .$$

We have explained how to compute each term's weight ("importance") in each document.
The intuitive formula for query score $f(q, d)$ is then
$$
f_D(q, d)\sim\sum_{t \in q}\text{tfidf}(t, d, D)\ .
$$
% TFIDF is called weighting scheme exactly for this reason. % toto urcite inam

This formula can be improved -- in this form, it favors longer queries $q$.
We can also precompute the TFIDF values for each document and vocabulary word in the corpus.
Thus we obtain $|D|\times {vocabulary\_size}$ matrix $V$ of TFIDF values.
To get the relevance of each document to the query $q$, that is $f_D(q, d)$, we first represent the query $q$ as a bag of words vector, corresponding to the columns of our precomputed TFIDF matrix $V$, ignoring words that appear in the query only. %only in the query?
The resulting score is then the normalized (not to favor long queries or documents) dot product of a row of the matrix and the BOW representation of the query $q$, denoted $\vec{q}$.
We can obtain the scores for every query document pair by matrix multiplication:
$$
f_D(q, d) = \frac{V\vec{q}}{|\vec{q}|} \in \R^{|D|}\ ,
$$
provided that matrix $V$ is row-normalized (euclidian norm of each row is equal to one).
Please note that this is equivalent to computing the cosine similarity for each document and query vector pair.

This function is one of the first and still widely used \citep{Beel2016} metrics for document retrieval. 

Some of the apparent disadvantages of TFIDF are the lack of semantic meaning that comes from the independence on the word order and matching exact words only. % independence???
The former may be improved by using n-grams and the latter using character level features, which is especially helpful in inflected languages such as Czech.
On the other hand, TFIDF is, to this day, a very well-performing low-computation cost ranking function.

Over the years, multiple versions of the TFIDF approach appeared, differing slightly in formulas or weights of the factors. One of such versions is Best Match 25.
% maybe put the last sentence as the last sentence of the whole section

\subsubsection{Best Match 25 (BM25)}

One of the best performing versions \citep{bm_vs_tfidf} is Best Match 25 (BM25) by \cite{bm25}.
The original formula is:
$$
\text{BM25}(q, d) = 
        \log \text{idf}(t, d)
        \cdot \frac{(k_1 + tf_{t, d})}{k_1((1-b) + b \cdot (L_d / L_{avg})) + tf_{t, d}} 
        \cdot \frac{(k_3 + 1)tf_{t, q}}{k_3 + tf_{t, q}}.
$$

The commonly used formula is
$$
\text{BM25}(Q,D)=\sum^n_{i=1}\text{IDF}(q_i)\frac{f(q_i, D)(k_1+1)}{f(q_i,D)+k_1\left(1-b+b\frac{\abs{D}}{\text{avdgl}}\right)}
\ ,$$
which differs from the original formula in several parameters (initially, four) due to setting these parameters to neutral values.

In the formula, $Q$ is a query consisting of words $q_1,\ldots,q_n$, $f(q_i,D)$ is a frequency of keyword $q_i$ in the document $D$, $|D|$ is a length of the document $D$. 

%TODO problem with long documents - BM25+ http://sifaka.cs.uiuc.edu/~ylv2/pub/cikm11-lowerbound.pdf

This approach does not account for word variations and the real meaning.
However, it, to this day, represents a solid baseline as reported by \cite{weak_baselines}.

\subsection{Neural Approaches}

When dealing with natural language processing, we can approach the task in two different ways, both described in detail by \cite{two-tower}. 

The first is to have the document-query pair $(d,q)$ on the input of the neural model (cross-attention model).
One of the benefits is the direct usage of the neural model on the downstream tasks, possibly granting better performance.

The second approach is to preprocess the whole database of documents by the model and then using some metric for choosing the documents related to the query based on the computed representation.
The obvious advantage is the offline preprocessing and thus improved performance during inference.
Only the unseen query has to be processed by the neural model followed by computing the metric between the documents' representation and the query representation.
This is usually way faster than using a neural model for every document and can also be significantly spedup \citep{faiss}.
The obvious disadvantage is the need for additional space to store the precomputed representation.

We will be using the second approach as our computational resources are limited.

% TODO under this line ---------------------------------
\subsubsection{BERT}
In 2018, BERT \citep{bert} the now well-established NLP model was introduced. The model is pre-trained using the masked language modeling task and the next sentence prediction task.
The defining feature of BERT is its self-attention mechanism, which attends to the whole input at once and transforms input tokens based on full context, both left, and right to the token.
Since the "each versus each" approach is employed, this mechanism naturally introduces $O(N^2)$ time complexity, which acts as a bottleneck in applications, where a longer input is required.
Since its introduction, there have been many well-performing models based on BERT (this movement is sometimes referred to as "BERTology").
In this thesis, we will focus on models which use similar architecture and modify attention mechanism in order to be able to compute long inputs.
% used usually for GLUE benchmark which is not???? for document retrieval
% TODO maybe include attention formula

\subsubsection{Longformer}

Longformer model \citep{longformer} arises when instead of computing the whole $QK^T$, only certain regions (usually specific diagonals) are calculated, thus reducing the model time complexity allowing for longer inputs.
Huggingface models are available.

\subsubsection{Reformer}

Reformer model \citep{reformer} offers two improvements to the transformer model.
The first is the usage of locality-sensitive hashing in the attention mechanism, which in order to approximate the full attention matrix focuses for each input token only on the closest tokens.
The closest tokens are found using the locality-sensitive hashing function.
This, coupled with memory saving features, offers a model which can have a whole novel as its single input.

\subsubsection{Performer}

Performer model \citep{performer} looks at the self-attention mechanism as a kernel function and introduces randomized feature functions such that the expected value of scalar product %????%e
of the resulting feature vectors is equal to the true value.
% TODO weak ensurences

\subsubsection{Nyostromformer}

Nyostromformer \citep{nystrom} uses "smart" Moore-Penrose inverse approximation. Instead of full $Q$ and $K$ matrices, only segment-means are used, and then the Moore-Penrose inverse is calculated using an iterative algorithm.

\subsubsection{ColBERT}


