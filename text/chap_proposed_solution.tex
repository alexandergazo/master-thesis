\chapter{Proposed Solutions}

\section{Baselines}

As the traditional baseline, we have decided to use BM25, as proposed by \citet{weak-baselines}.
We use the Pyserini\footnote{\url{https://github.com/castorini/pyserini/}} python toolkit for information retrieval \citep{pyserini} -- a python interface for the Anserini\footnote{\url{https://github.com/castorini/Anserini}} library \citep{anserini1, anserini2}, originally implemented in Java, building on the Lucene\footnote{\url{https://lucene.apache.org/}} library.
The Pyserini library is a highly optimized library implementing multiple search algorithms. In Pyserini, BM25's implemention uses bigrams features. 

We also compare our results with the results achieved by \citet{rypar}, who researched document retrieval for documents up to one paragraph long ($\approx$ 230 tokens), using mBERT and ColBERT \citep{colbert} models.

\section{Neural Models}

From the models described in Section \ref{subsec:bertology} only Longformer \citep{longformer}, BigBird \citep{bigbird} and Reformer \citep{reformer} are available in the HuggingFace Transformers library \citep{huggingface}, and none are trained in the Czech language. 

We have chosen to experiment with BigBird and \nystr{}. 
We chose BigBird over Longformer, as the BigBird architecture essentially performs the same way as Longformer. Another reason is that during experimentation with Longformer, we ran into scalability issues with embedding the documents.
The time required to generate the representation of the entire Czech Wikipedia was estimated to take $\approx$ 300 hours (for the \CTK{} infobank $\approx 1500$ hours), despite using the authors' guide\footnote{\url{https://github.com/allenai/longformer/blob/caefee668e39cacdece7dd603a0bebf24df6d8ca/scripts/convert_model_to_long.ipynb}} and implementation.

The Performer \citep{performer}, Linformer \citep{linformer}, and Reformer \citep{reformer} models, although promising, were shown to be outperformed by the \nystr{} model by its authors (see Appendix 2).
Given the resource intensity of the task\footnote{linear slow-down in an ideal scenario means that for sequences of length 2048 or 4096 tokens the model nevertheless performs $\approx$ 8 to 16 times slower} and the lack of a unified code base, we have decided to follow the results and implement the \nystr{} model only.

We tested three approaches to the task:

\begin{enumerate}
    \item Train the BigBird model using multilingual knowledge distillation as described in Section \ref{subsec:distillation} using the SBERT library \citep{sbert}.
    \item Train \nystr{} on MLM task from RobeCzech \citep{robeczech} checkpoint using the provided code base\footnote{\url{https://github.com/mlpen/Nystromformer/}}.
    \item As a soft baseline, evaluate the pre-trained RobeCzech model and SBERT-fine-tuned (see Figure \ref{fig:sbert_architectures}) RobeCzech model on the document retrieval task.  
\end{enumerate}

Following the discussion in Section \ref{sec:data_length}, we have chosen the input length of the long models to be 2048. 

\section{Evaluation Metrics}

Document retrieval is best evaluated by the model's ability to match the evidence set with the retrieved documents.
By the retrieved documents, we mean the sequence of $k$ documents from the knowledge base with the highest scores.
We can match the evidence set in a variety of ways, motivating multiple measures of performance:
\begin{itemize}
    \item \textbf{Precision} - We try to ``fit within'' the evidence set with retrieved documents.
    \item \textbf{Recall} - We try to ``cover'' the evidence set.
    \item \textbf{F1} - The harmonic mean of precision and recall. 
    \item \textbf{Mean Reciprocal Rank (MRR)} \citep{mrr} - We try to retrieve the relevant documents at the first positions. 
\end{itemize}
The specific formulas are: 
\begin{equation}
    \text{precision} = \frac{|\{\text{evidence documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{retrieved documents}\}|}\ ,
\end{equation}
\begin{equation}
    \text{recall} = \frac{|\{\text{evidence documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{evidence documents}\}|}\ ,
\end{equation}
\begin{equation}
    \text{F1} = 
    %\frac{2}{\frac{1}{\text{precision}}+\frac{1}{\text{recall}}} =
    \frac{2\cdot\text{precision}\cdot\text{recall}}{\text{precision}+\text{recall}}\ ,
\end{equation}
\begin{equation}
    \text{MRR} = \frac{1}{|Q|}\sum_{i=1}^{|Q|}{\frac{1}{\text{rank}_i}}\ ,
\end{equation}
where $Q$ is a sequence of queries and $\text{rank}_i$ is the rank of the first relevant document retrieved for the query $q_i$.

The most relevant metrics for us will be the recall and the MRR since we aim to provide all the necessary knowledge ideally among the first results.
Since the precision and the recall depend on the number $k$ of retrieved documents, we evaluate the models with varying $k$ and denote each evaluation with the suffix $\text{@}k$.